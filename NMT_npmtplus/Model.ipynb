{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Building Encoder\"\"\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self,input_dim,embed_dim,hidden_dim,segment_dim,n_layers,dropout,segment_threshold,device):\n",
    "    super().__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.n_layers = n_layers\n",
    "    self.segment_threshold = segment_threshold\n",
    "    self.segment_dim = segment_dim\n",
    "    self.device = device\n",
    "    \n",
    "    self.embedding = nn.Embedding(input_dim,embed_dim)\n",
    "    self.rnn = nn.GRU(embed_dim,hidden_dim,n_layers,dropout=dropout,bidirectional=True)\n",
    "\n",
    "    self.segmentRnn = nn.GRU(hidden_dim*2,segment_dim,n_layers,dropout=dropout)\n",
    "    # self.fc = nn.Linear(hidden_dim*2,hidden_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self,input):\n",
    "\n",
    "    #input = [src len, batch size]\n",
    "    embedded = self.dropout(self.embedding(input))\n",
    "    #embedded = [src len, batch size, emb dim]\n",
    "\n",
    "    outputs, hidden = self.rnn(embedded)\n",
    "    #outputs = [src len, batch size, hid dim * num directions]\n",
    "    #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "    segment_encoding, hidden = self.segment_rnn(outputs)\n",
    "    #segment_encoding = [src len* (src len+1)/2, batch size, segment_dim*num_directions]\n",
    "    #hidden = [n layers * num_directions, batch size, hid dim]\n",
    "\n",
    "    # hidden = torch.tanh(self.fc(torch.cat((hidden[-2],hidden[-1]),dim=1)))\n",
    "\n",
    "    return segment_encoding,hidden\n",
    "\n",
    "  def segment_rnn(self,outputs):\n",
    "    N = outputs.shape[0]\n",
    "    print('outputs :', outputs.shape)\n",
    "    batch_size = outputs.shape[1]\n",
    "    dp_forward = torch.zeros(N, N, batch_size, self.segment_dim).to(self.device)\n",
    "    dp_backward = torch.zeros(N, N, batch_size, self.segment_dim).to(self.device)\n",
    "    print('dp_forward :', dp_forward.shape)\n",
    "    print('dp_backward :', dp_backward.shape)\n",
    "\n",
    "    for i in range(N):\n",
    "      hidden_forward = torch.randn(self.n_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "      print('hidden_forward :', hidden_forward.shape)\n",
    "      print('f condition :', i,',', min(N, i + self.segment_threshold))\n",
    "      for j in range(i, min(N, i + self.segment_threshold)):\n",
    "        \n",
    "        # outputs[j] = [batch size, hidden_dim* num_direction]\n",
    "        next_input = outputs[j].unsqueeze(0)\n",
    "        print('f next_input :', next_input.shape)\n",
    "        # next_input = [1, batch size, hidden_dim* num_direction]\n",
    "        \n",
    "        out, hidden_forward = self.segmentRnn(next_input,hidden_forward)\n",
    "        print('f out :', out.shape)\n",
    "        #out = [1, batch size, segment_dim]\n",
    "        #hidden_forward = [n layers , batch size, hid dim]\n",
    "        print('f out squeeze:', out.squeeze(0).shape)\n",
    "\n",
    "        dp_forward[i][j] = out.squeeze(0)\n",
    "\n",
    "    for i in range(N):\n",
    "      hidden_backward = torch.randn(self.n_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "      print('hidden_backward :', hidden_backward.shape)\n",
    "      print('b condition :',i,',', max(-1, i - self.segment_threshold))\n",
    "      for j in range(i, max(-1, i - self.segment_threshold), -1):\n",
    "\n",
    "        # outputs[j] = [batch size, hidden_dim* num_direction]\n",
    "        next_input = outputs[j].unsqueeze(0)\n",
    "        # next_input = [1, batch size, hidden_dim* num_direction]\n",
    "        \n",
    "        out, hidden_backward = self.segmentRnn(next_input,hidden_backward)\n",
    "        #out = [1, batch size, segment_dim]\n",
    "        #hidden_backward = [n layers , batch size, hid dim]\n",
    "        \n",
    "        dp_backward[j][i] = out.squeeze(0)\n",
    "    \n",
    "    dp = torch.cat((dp_forward,dp_backward),dim=3)\n",
    "    dp = dp[torch.triu(torch.ones(N, N)) == 1]\n",
    "    return dp,torch.cat((hidden_forward,hidden_backward),dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 64\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "segment_dim = 128\n",
    "n_layers = 2\n",
    "dropout = 0.4\n",
    "segment_threshold = 4\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(input_dim, embed_dim, hidden_dim, segment_dim, n_layers, dropout, segment_threshold, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding): Embedding(64, 128)\n",
       "  (rnn): GRU(128, 128, num_layers=2, dropout=0.4, bidirectional=True)\n",
       "  (segmentRnn): GRU(256, 128, num_layers=2, dropout=0.4)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.randint(50, size=(4,96))\n",
    "inputs = torch.from_numpy(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs : torch.Size([4, 96, 256])\n",
      "dp_forward : torch.Size([4, 4, 96, 128])\n",
      "dp_backward : torch.Size([4, 4, 96, 128])\n",
      "hidden_forward : torch.Size([2, 96, 128])\n",
      "f condition : 0 , 4\n",
      "f next_input : torch.Size([1, 96, 256])\n",
      "f out : torch.Size([1, 96, 128])\n",
      "f out squeeze: torch.Size([96, 128])\n",
      "f next_input : torch.Size([1, 96, 256])\n",
      "f out : torch.Size([1, 96, 128])\n",
      "f out squeeze: torch.Size([96, 128])\n",
      "f next_input : torch.Size([1, 96, 256])\n",
      "f out : torch.Size([1, 96, 128])\n",
      "f out squeeze: torch.Size([96, 128])\n",
      "f next_input : torch.Size([1, 96, 256])\n",
      "f out : torch.Size([1, 96, 128])\n",
      "f out squeeze: torch.Size([96, 128])\n",
      "hidden_forward : torch.Size([2, 96, 128])\n",
      "f condition : 1 , 4\n",
      "f next_input : torch.Size([1, 96, 256])\n",
      "f out : torch.Size([1, 96, 128])\n",
      "f out squeeze: torch.Size([96, 128])\n",
      "f next_input : torch.Size([1, 96, 256])\n",
      "f out : torch.Size([1, 96, 128])\n",
      "f out squeeze: torch.Size([96, 128])\n",
      "f next_input : torch.Size([1, 96, 256])\n",
      "f out : torch.Size([1, 96, 128])\n",
      "f out squeeze: torch.Size([96, 128])\n",
      "hidden_forward : torch.Size([2, 96, 128])\n",
      "f condition : 2 , 4\n",
      "f next_input : torch.Size([1, 96, 256])\n",
      "f out : torch.Size([1, 96, 128])\n",
      "f out squeeze: torch.Size([96, 128])\n",
      "f next_input : torch.Size([1, 96, 256])\n",
      "f out : torch.Size([1, 96, 128])\n",
      "f out squeeze: torch.Size([96, 128])\n",
      "hidden_forward : torch.Size([2, 96, 128])\n",
      "f condition : 3 , 4\n",
      "f next_input : torch.Size([1, 96, 256])\n",
      "f out : torch.Size([1, 96, 128])\n",
      "f out squeeze: torch.Size([96, 128])\n",
      "hidden_backward : torch.Size([2, 96, 128])\n",
      "b condition : 0 , -1\n",
      "hidden_backward : torch.Size([2, 96, 128])\n",
      "b condition : 1 , -1\n",
      "hidden_backward : torch.Size([2, 96, 128])\n",
      "b condition : 2 , -1\n",
      "hidden_backward : torch.Size([2, 96, 128])\n",
      "b condition : 3 , -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.6903, -1.0416,  0.1952,  ..., -0.2684,  0.8140,  0.0920],\n",
       "          [-0.1308, -0.1469,  0.3786,  ...,  0.1485, -0.4135, -0.0331],\n",
       "          [-0.3406, -0.1709, -0.5119,  ...,  0.1385, -0.2987,  0.0240],\n",
       "          ...,\n",
       "          [-0.1609,  0.6451,  0.2526,  ..., -0.2893, -0.2642, -0.0646],\n",
       "          [-0.5268, -0.5851, -0.6335,  ...,  1.7021,  0.0222, -0.8598],\n",
       "          [ 0.9743, -0.1840, -0.6940,  ...,  0.3823,  1.1701, -0.5797]],\n",
       " \n",
       "         [[ 1.1594, -0.5827,  0.0364,  ..., -0.3593, -0.0909, -0.3255],\n",
       "          [-0.0824,  0.0204,  0.2786,  ..., -0.2978, -0.4635, -0.4056],\n",
       "          [-0.1682, -0.0154, -0.0692,  ..., -0.2043, -0.5647,  0.1498],\n",
       "          ...,\n",
       "          [-0.0562,  0.2657,  0.1794,  ..., -0.0520, -0.4269,  0.3653],\n",
       "          [-0.3715, -0.1650, -0.5096,  ..., -0.4506,  0.1396, -0.0742],\n",
       "          [ 0.7220, -0.2693, -0.3038,  ..., -0.2139, -0.8202, -0.0259]],\n",
       " \n",
       "         [[ 0.5950, -0.3648,  0.0765,  ...,  0.0560, -0.3390, -0.1510],\n",
       "          [ 0.0426,  0.0567,  0.1853,  ...,  0.2949,  0.0753,  0.2443],\n",
       "          [-0.0676, -0.0607,  0.0442,  ...,  0.0285,  0.0412, -0.2280],\n",
       "          ...,\n",
       "          [ 0.1225,  0.2088, -0.0138,  ...,  0.1325,  0.4412, -0.1967],\n",
       "          [-0.2364, -0.0292, -0.4123,  ..., -0.2760,  0.1916,  1.0042],\n",
       "          [ 0.4349, -0.2868, -0.1108,  ...,  0.0508, -0.5531, -0.0359]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0810,  0.6741,  0.0499,  ..., -0.1774, -0.3545, -0.0728],\n",
       "          [-0.4142,  0.7183, -0.7313,  ...,  0.6004,  0.3645,  0.4823],\n",
       "          [ 0.0101,  0.0268,  0.9157,  ...,  0.1528, -0.3457, -0.0498],\n",
       "          ...,\n",
       "          [-0.4394, -0.6706,  0.1612,  ...,  1.1600,  0.5122, -0.3699],\n",
       "          [ 0.9044,  0.1522,  0.5828,  ..., -0.4180,  0.5050,  1.9294],\n",
       "          [-0.0840,  0.6585, -0.5086,  ..., -0.0830, -0.4397, -0.4041]],\n",
       " \n",
       "         [[-0.0993,  0.3750, -0.1389,  ..., -0.3293, -0.0429,  0.5861],\n",
       "          [-0.0955,  0.5569, -0.4642,  ..., -0.1690,  0.3350,  0.8294],\n",
       "          [-0.0429,  0.1142,  0.6810,  ..., -0.2721,  0.1387,  0.7410],\n",
       "          ...,\n",
       "          [-0.3309, -0.1848,  0.1875,  ..., -0.5440, -0.4641,  0.0787],\n",
       "          [ 0.5687,  0.0839,  0.4040,  ..., -0.1791, -0.3937, -0.2961],\n",
       "          [ 0.0663,  0.4720, -0.3108,  ...,  0.2143,  0.1115, -0.9438]],\n",
       " \n",
       "         [[ 0.3324, -0.0528, -0.2497,  ..., -0.5812,  0.2737,  1.1020],\n",
       "          [ 0.7080, -0.1126,  0.1407,  ..., -0.2691,  0.4706,  1.2423],\n",
       "          [-0.7615, -0.3402, -0.3295,  ..., -0.5780,  0.1325,  1.2625],\n",
       "          ...,\n",
       "          [ 0.5084,  0.6799, -0.0749,  ..., -0.7668, -0.4167,  0.0910],\n",
       "          [ 0.8233, -0.2497,  0.6709,  ...,  0.0482, -0.4507, -0.4043],\n",
       "          [-0.0737, -0.1023,  0.4480,  ...,  0.2462,  0.3912, -1.2213]]],\n",
       "        grad_fn=<IndexBackward>),\n",
       " tensor([[[-0.4713, -0.2820,  0.4908,  ..., -0.2290, -0.2466, -0.0668],\n",
       "          [-0.5027, -0.3609, -0.0512,  ..., -0.0475, -0.1147, -0.0548],\n",
       "          [ 0.4395,  0.7701,  1.1794,  ...,  0.0824, -0.1255, -0.0775],\n",
       "          ...,\n",
       "          [-0.6840, -0.1780,  0.2940,  ..., -0.1523, -0.4930, -0.1186],\n",
       "          [ 0.4096,  0.0727, -0.3756,  ..., -0.0026,  0.0467, -0.3751],\n",
       "          [-0.4282,  1.1385,  0.6087,  ...,  0.0379, -0.1742,  0.2894]],\n",
       " \n",
       "         [[ 0.3324, -0.0528, -0.2497,  ..., -0.0060, -0.2836,  0.2393],\n",
       "          [ 0.7080, -0.1126,  0.1407,  ..., -0.0370, -0.0759,  0.3724],\n",
       "          [-0.7615, -0.3402, -0.3295,  ..., -0.0692, -0.0965,  0.1023],\n",
       "          ...,\n",
       "          [ 0.5084,  0.6799, -0.0749,  ..., -0.3955, -0.1406,  0.1350],\n",
       "          [ 0.8233, -0.2497,  0.6709,  ..., -0.3904, -0.2004, -0.1569],\n",
       "          [-0.0737, -0.1023,  0.4480,  ...,  0.0850, -0.1350, -0.4163]]],\n",
       "        grad_fn=<CatBackward>))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('tf2_env': venv)",
   "language": "python",
   "name": "python37564bittf2envvenv9baad5f63bf544adb456767f347f0830"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
