{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from typing import Dict, Sequence, Text\n",
    "\n",
    "from absl import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### https://github.com/google/flax/tree/master/examples/sst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(datasets: Sequence[tf.data.Dataset],\n",
    "                special_tokens: Sequence[Text] = (b'<pad>', b'<unk>', b'<s>', b'</s>'),\n",
    "                min_freq: int = 0) -> Dict[Text, int]:\n",
    "  \"\"\"Returns a vocabulary of tokens with optional minimum frequency.\"\"\"\n",
    "  # Count the tokens in the datasets.\n",
    "  counter = collections.Counter()\n",
    "  for dataset in datasets:\n",
    "    for example in tfds.as_numpy(dataset):\n",
    "      counter.update(whitespace_tokenize(example['sentence']))\n",
    "\n",
    "  # Add special tokens to the start of vocab.\n",
    "  vocab = collections.OrderedDict()\n",
    "  for token in special_tokens:\n",
    "    vocab[token] = len(vocab)\n",
    "\n",
    "  # Add all other tokens to the vocab if their frequency is >= min_freq.\n",
    "  for token in sorted(list(counter.keys())):\n",
    "    if counter[token] >= min_freq:\n",
    "      vocab[token] = len(vocab)\n",
    "\n",
    "  logging.info('Number of unfiltered tokens: %d', len(counter))\n",
    "  logging.info('Vocabulary size: %d', len(vocab))\n",
    "  return vocab\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text: Text) -> Sequence[Text]:\n",
    "  \"\"\"Splits an input into tokens by whitespace.\"\"\"\n",
    "  return text.strip().split()\n",
    "\n",
    "\n",
    "def get_shuffled_batches(dataset: tf.data.Dataset,\n",
    "                         seed: int = 0,\n",
    "                         batch_size: int = 64) -> tf.data.Dataset:\n",
    "  \"\"\"Returns a Dataset that consists of padded batches when iterated over.\n",
    "  This shuffles the examples randomly each epoch. The random order is\n",
    "  deterministic and controlled by the seed.\n",
    "  Batches are padded because sentences have different lengths.\n",
    "  Sentences that are shorter in a batch will get 0s added at the end, until\n",
    "  all sentences in the batch have the same length.\n",
    "  Args:\n",
    "    dataset: A TF Dataset with examples to be shuffled and batched.\n",
    "    seed: The seed that determines the shuffling order, with a different order\n",
    "      each epoch.\n",
    "    batch_size: The size of each batch. The remainder is dropped.\n",
    "  Returns:\n",
    "    A TF Dataset containing padded batches.\n",
    "  \"\"\"\n",
    "  # For shuffling we need to know how many training examples we have.\n",
    "  num_examples = dataset.reduce(np.int64(0), lambda x, _: x + 1).numpy()\n",
    "\n",
    "  # `padded_shapes` says what kind of shapes to expect: [] means a scalar, [-1]\n",
    "  # means a vector of variable length, and [1] means a vector of size 1.\n",
    "  return dataset.shuffle(\n",
    "      num_examples, seed=seed, reshuffle_each_iteration=True).padded_batch(\n",
    "          batch_size,\n",
    "          padded_shapes={\n",
    "              'idx': [],\n",
    "              'sentence': [-1],\n",
    "              'label': [1],\n",
    "              'length': []\n",
    "          },\n",
    "          drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "def get_batches(dataset: tf.data.Dataset,\n",
    "                batch_size: int = 64) -> tf.data.Dataset:\n",
    "  \"\"\"Returns a Dataset that consists of padded batches when iterated over.\"\"\"\n",
    "  return dataset.padded_batch(\n",
    "      batch_size,\n",
    "      padded_shapes={\n",
    "          'idx': [],\n",
    "          'sentence': [-1],\n",
    "          'label': [1],\n",
    "          'length': []\n",
    "      },\n",
    "      drop_remainder=False).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "class SST2DataSource:\n",
    "  \"\"\"Provides SST-2 data as pre-processed batches, a vocab, and embeddings.\"\"\"\n",
    "  # pylint: disable=too-few-public-methods\n",
    "\n",
    "  def __init__(self, min_freq: int = 0):\n",
    "    # Load datasets.\n",
    "    data = tfds.load('glue/sst2')\n",
    "    train_raw = data['train']\n",
    "    valid_raw = data['validation']\n",
    "    test_raw = data['test']\n",
    "\n",
    "    # Print an example.\n",
    "    logging.info('Data sample: %s', next(tfds.as_numpy(train_raw.skip(4))))\n",
    "\n",
    "    # Get a vocabulary and a corresponding GloVe word embedding matrix.\n",
    "    vocab = build_vocab((train_raw,), min_freq=min_freq)\n",
    "\n",
    "    unk_idx = vocab[b'<unk>']\n",
    "    bos_idx = vocab[b'<s>']\n",
    "    eos_idx = vocab[b'</s>']\n",
    "\n",
    "    # Turn data examples into pre-processed examples by turning each sentence\n",
    "    # into a sequence of token IDs. Also pre-prepend a beginning-of-sequence\n",
    "    # token <s> and append an end-of-sequence token </s>.\n",
    "\n",
    "    def tokenize(text: tf.Tensor):\n",
    "      \"\"\"Whitespace tokenize text.\"\"\"\n",
    "      return [whitespace_tokenize(text.numpy())]\n",
    "\n",
    "    def tf_tokenize(text: tf.Tensor):\n",
    "      return tf.py_function(tokenize, [text], Tout=tf.string)\n",
    "\n",
    "    def encode(tokens: tf.Tensor):\n",
    "      \"\"\"Encodes a sequence of tokens (strings) into a sequence of token IDs.\"\"\"\n",
    "      return [[vocab[t] if t in vocab else unk_idx for t in tokens.numpy()]]\n",
    "\n",
    "    def tf_encode(tokens: tf.Tensor):\n",
    "      \"\"\"Maps tokens to token IDs.\"\"\"\n",
    "      return tf.py_function(encode, [tokens], Tout=tf.int64)\n",
    "\n",
    "    def tf_wrap_sequence(sequence: tf.Tensor):\n",
    "      \"\"\"Prepends BOS ID and appends EOS ID to a sequence of token IDs.\"\"\"\n",
    "      return tf.concat(([bos_idx], tf.concat((sequence, [eos_idx]), 0)), 0)\n",
    "\n",
    "    def preprocess_example(example: Dict[Text, tf.Tensor]):\n",
    "      example['sentence'] = tf_wrap_sequence(\n",
    "          tf_encode(tf_tokenize(example['sentence'])))\n",
    "      example['label'] = [example['label']]\n",
    "      example['length'] = tf.shape(example['sentence'])[0]\n",
    "      return example\n",
    "\n",
    "    self.preprocess_fn = preprocess_example\n",
    "\n",
    "    # Pre-process all datasets.\n",
    "    self.train_dataset = train_raw.map(preprocess_example).cache()\n",
    "    self.valid_dataset = valid_raw.map(preprocess_example).cache()\n",
    "    self.test_dataset = test_raw.map(preprocess_example).cache()\n",
    "\n",
    "    self.valid_raw = valid_raw\n",
    "    self.test_raw = test_raw\n",
    "\n",
    "    self.vocab = vocab\n",
    "    self.vocab_size = len(vocab)\n",
    "\n",
    "    self.unk_idx = unk_idx\n",
    "    self.bos_idx = bos_idx\n",
    "    self.eos_idx = eos_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LSTM classifier model for SST-2.\"\"\"\n",
    "\n",
    "import functools\n",
    "from typing import Any, Callable, Dict, Text\n",
    "\n",
    "import flax\n",
    "from flax import nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# pylint: disable=arguments-differ,too-many-arguments\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=(0, 1, 2, 3))\n",
    "def create_model(seed: int, batch_size: int, max_len: int,\n",
    "                 model_kwargs: Dict[Text, Any]):\n",
    "  \"\"\"Instantiates a new model.\"\"\"\n",
    "  module = TextClassifier.partial(train=False, **model_kwargs)\n",
    "  _, initial_params = module.init_by_shape(\n",
    "      jax.random.PRNGKey(seed),\n",
    "      [((batch_size, max_len), jnp.int32),\n",
    "       ((batch_size,), jnp.int32)])\n",
    "  model = nn.Model(module, initial_params)\n",
    "  return model\n",
    "\n",
    "\n",
    "def word_dropout(inputs: jnp.ndarray, rate: float, unk_idx: int, \n",
    "        deterministic: bool = False):\n",
    "  \"\"\"Replaces a fraction (rate) of inputs with <unk>.\"\"\"\n",
    "  if deterministic or rate == 0.:\n",
    "    return inputs\n",
    "\n",
    "  mask = jax.random.bernoulli(nn.make_rng(), p=rate, shape=inputs.shape)\n",
    "  return jnp.where(mask, jnp.array([unk_idx]), inputs)\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "  \"\"\"Embedding Module.\"\"\"\n",
    "\n",
    "  def apply(self,\n",
    "            inputs: jnp.ndarray,\n",
    "            num_embeddings: int,\n",
    "            features: int,\n",
    "            emb_init: Callable[...,\n",
    "                               np.ndarray] = nn.initializers.normal(stddev=0.1),\n",
    "            frozen: bool = False):\n",
    "    # inputs.shape = <int64>[batch_size, seq_length]\n",
    "    embedding = self.param('embedding', (num_embeddings, features), emb_init)\n",
    "    embed = jnp.take(embedding, inputs, axis=0)\n",
    "    if frozen:  # Keep the embeddings fixed at initial (pretrained) values.\n",
    "      embed = lax.stop_gradient(embed)\n",
    "    return embed\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "  \"\"\"LSTM encoder. Turns a sequence of vectors into a vector.\"\"\"\n",
    "\n",
    "  def apply(self,\n",
    "            inputs: jnp.ndarray,\n",
    "            lengths: jnp.ndarray,\n",
    "            hidden_size: int = None):\n",
    "    # inputs.shape = <float32>[batch_size, seq_length, emb_size].\n",
    "    # lengths.shape = <int64>[batch_size,]\n",
    "    batch_size = inputs.shape[0]\n",
    "    carry = nn.LSTMCell.initialize_carry(\n",
    "        jax.random.PRNGKey(0), (batch_size,), hidden_size)\n",
    "    _, outputs = flax.jax_utils.scan_in_dim(\n",
    "        nn.LSTMCell.partial(name='lstm_cell'), carry, inputs, axis=1)\n",
    "    return outputs[jnp.arange(batch_size), jnp.maximum(0, lengths - 1), :]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  \"\"\"A 2-layer MLP.\"\"\"\n",
    "\n",
    "  def apply(self,\n",
    "            inputs: jnp.ndarray,\n",
    "            hidden_size: int = None,\n",
    "            output_size: int = None,\n",
    "            output_bias: bool = False,\n",
    "            dropout: float = None,\n",
    "            train: bool = None):\n",
    "    # inputs.shape = <float32>[batch_size, seq_length, hidden_size]\n",
    "    hidden = nn.Dense(inputs, hidden_size, name='hidden')\n",
    "    hidden = nn.tanh(hidden)\n",
    "    if train:\n",
    "      hidden = nn.dropout(hidden, rate=dropout)\n",
    "    output = nn.Dense(hidden, output_size, bias=output_bias, name='output')\n",
    "    return output\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "  \"\"\"LSTM classifier.\"\"\"\n",
    "\n",
    "  def apply(self,\n",
    "            embed: jnp.ndarray,\n",
    "            lengths: jnp.ndarray,\n",
    "            hidden_size: int = None,\n",
    "            output_size: int = None,\n",
    "            dropout: float = None,\n",
    "            emb_dropout: float = None,\n",
    "            train: bool = None):\n",
    "    \"\"\"Encodes the input sequence and makes a prediction using an MLP.\"\"\"\n",
    "    # embed <float32>[batch_size, seq_length, embedding_size]\n",
    "    # lengths <int64>[batch_size]\n",
    "    if train:\n",
    "      embed = nn.dropout(embed, rate=emb_dropout)\n",
    "\n",
    "    # Encode the sequence of embedding using an LSTM.\n",
    "    hidden = LSTM(embed, lengths, hidden_size=hidden_size, name='lstm')\n",
    "    if train:\n",
    "      hidden = nn.dropout(hidden, rate=dropout)\n",
    "\n",
    "    # Predict the class using an MLP.\n",
    "    logits = MLP(\n",
    "        hidden,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=output_size,\n",
    "        output_bias=False,\n",
    "        dropout=dropout,\n",
    "        name='mlp',\n",
    "        train=train)\n",
    "    return logits\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "  \"\"\"Full classification model.\"\"\"\n",
    "\n",
    "  def apply(self,\n",
    "            inputs: jnp.ndarray,\n",
    "            lengths: jnp.ndarray,\n",
    "            unk_idx: int = 1,\n",
    "            vocab_size: int = None,\n",
    "            embedding_size: int = None,\n",
    "            word_dropout_rate: float = None,\n",
    "            freeze_embeddings: bool = None,\n",
    "            train: bool = False,\n",
    "            emb_init: Callable[..., Any] = nn.initializers.normal(stddev=0.1),\n",
    "            **kwargs):\n",
    "    # Apply word dropout.\n",
    "    if train:\n",
    "      inputs = word_dropout(inputs, rate=word_dropout_rate, unk_idx=unk_idx)\n",
    "\n",
    "    # Embed the inputs.\n",
    "    embed = Embedding(\n",
    "        inputs,\n",
    "        vocab_size,\n",
    "        embedding_size,\n",
    "        emb_init=emb_init,\n",
    "        frozen=freeze_embeddings,\n",
    "        name='embed')\n",
    "\n",
    "    # Encode with LSTM and classify.\n",
    "    logits = LSTMClassifier(\n",
    "        embed, lengths, train=train, name='lstm_classifier', **kwargs)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Any, Dict, Text, Tuple\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import flax\n",
    "import flax.training.checkpoints\n",
    "from flax import nn\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.compat.v2.io import gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    'learning_rate', default=0.0005,\n",
    "    help=('The learning rate for the Adam optimizer.'))\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'batch_size', default=64,\n",
    "    help=('Batch size for training.'))\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'num_epochs', default=20,\n",
    "    help=('Number of training epochs.'))\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    'dropout', default=0.5,\n",
    "    help=('Dropout rate'))\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    'emb_dropout', default=0.5,\n",
    "    help=('Embedding dropout rate'))\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    'word_dropout_rate', default=0.1,\n",
    "    help=('Word dropout rate. Replaces input words with <unk>.'))\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'model_dir', default='output_dir',\n",
    "    help=('Directory to store model data'))\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'hidden_size', default=256,\n",
    "    help=('Hidden size for the LSTM and MLP.'))\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'embedding_size', default=256,\n",
    "    help=('Size of the word embeddings.'))\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'max_seq_len', default=55,\n",
    "    help=('Maximum sequence length in the dataset.'))\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'min_freq', default=5,\n",
    "    help=('Minimum frequency for training set words to be in the vocabulary.'))\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    'l2_reg', default=1e-6,\n",
    "    help=('L2 regularization weight'))\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'seed', default=0,\n",
    "    help=('Random seed for network initialization.'))\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'checkpoints_to_keep', default=1,\n",
    "    help=('How many checkpoints to keep. Default: 1 (keep best model only)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.vmap\n",
    "def binary_cross_entropy_loss(logit: jnp.ndarray, label: jnp.ndarray):\n",
    "  \"\"\"Numerically stable binary cross entropy loss.\n",
    "  This function is vmapped, so it is written for a single example, but can\n",
    "  handle a batch of examples.\n",
    "  Args:\n",
    "    logit: The output logits.\n",
    "    label: The correct labels.\n",
    "  Returns:\n",
    "    The binary cross entropy loss for each given logit.\n",
    "  \"\"\"\n",
    "  return label * nn.softplus(-logit) + (1 - label) * nn.softplus(logit)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(optimizer: Any, inputs: jnp.ndarray, lengths: jnp.ndarray,\n",
    "               labels: jnp.ndarray, rng: Any, l2_reg: float):\n",
    "  \"\"\"Single optimized training step.\n",
    "  Args:\n",
    "    optimizer: The optimizer to use to update the weights.\n",
    "    inputs: A batch of inputs. <int64>[batch_size, seq_len]\n",
    "    lengths: The lengths of the sequences in the batch. <int64>[batch_size]\n",
    "    labels: The labels of the sequences in the batch. <int64>[batch_size, 1]\n",
    "    rng: Random number generator for dropout.\n",
    "    l2_reg: L2 regularization weight.\n",
    "  Returns:\n",
    "    optimizer: The optimizer in its new state.\n",
    "    loss: The loss for this step.\n",
    "  \"\"\"\n",
    "  rng, new_rng = jax.random.split(rng)\n",
    "  def loss_fn(model):\n",
    "    with nn.stochastic(rng):\n",
    "      logits = model(inputs, lengths, train=True)\n",
    "    loss = jnp.mean(binary_cross_entropy_loss(logits, labels))\n",
    "\n",
    "    # L2 regularization\n",
    "    l2_params = jax.tree_leaves(model.params['lstm_classifier'])\n",
    "    l2_weight = jnp.sum([jnp.sum(p ** 2) for p in l2_params])\n",
    "    l2_penalty = l2_reg * l2_weight\n",
    "\n",
    "    loss = loss + l2_penalty\n",
    "    return loss, logits\n",
    "\n",
    "  loss, _, grad = optimizer.compute_gradient(loss_fn)\n",
    "  optimizer = optimizer.apply_gradient(grad)\n",
    "  return optimizer, loss, new_rng\n",
    "\n",
    "\n",
    "def get_predictions(logits: jnp.ndarray) -> jnp.ndarray:\n",
    "  \"\"\"Returns predictions given a batch of logits.\"\"\"\n",
    "  outputs = jax.nn.sigmoid(logits)\n",
    "  return (outputs > 0.5).astype(jnp.int32)\n",
    "\n",
    "\n",
    "def get_num_correct(logits: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
    "  \"\"\"Returns the number of correct predictions.\"\"\"\n",
    "  return jnp.sum(get_predictions(logits) == labels)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(model: nn.Module, inputs: jnp.ndarray, lengths: jnp.ndarray,\n",
    "              labels: jnp.ndarray):\n",
    "  \"\"\"A single evaluation step.\n",
    "  Args:\n",
    "    model: The model to be used for this evaluation step.\n",
    "    inputs: A batch of inputs. <int64>[batch_size, seq_len]\n",
    "    lengths: The lengths of the sequences in the batch. <int64>[batch_size]\n",
    "    labels: The labels of the sequences in the batch. <int64>[batch_size, 1]\n",
    "  Returns:\n",
    "    loss: The summed loss on this batch.\n",
    "    num_correct: The number of correct predictions in this batch.\n",
    "  \"\"\"\n",
    "  logits = model(inputs, lengths, train=False)\n",
    "  loss = jnp.sum(binary_cross_entropy_loss(logits, labels))\n",
    "  num_correct = get_num_correct(logits, labels)\n",
    "  return loss, num_correct\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Model, dataset: tf.data.Dataset):\n",
    "  \"\"\"Evaluates the model on a dataset.\n",
    "  Args:\n",
    "    model: A model to be evaluated.\n",
    "    dataset: A dataset to be used for the evaluation. Typically valid or test.\n",
    "  Returns:\n",
    "    A dict with the evaluation results.\n",
    "  \"\"\"\n",
    "  count = 0\n",
    "  total_loss = 0.\n",
    "  total_correct = 0\n",
    "\n",
    "  for ex in tfds.as_numpy(dataset):\n",
    "    inputs, lengths, labels = ex['sentence'], ex['length'], ex['label']\n",
    "    count = count + inputs.shape[0]\n",
    "    loss, num_correct = eval_step(model, inputs, lengths, labels)\n",
    "    total_loss += loss.item()\n",
    "    total_correct += num_correct.item()\n",
    "\n",
    "  loss = total_loss / count\n",
    "  accuracy = 100. * total_correct / count\n",
    "  metrics = dict(loss=loss, acc=accuracy)\n",
    "\n",
    "  return metrics\n",
    "\n",
    "\n",
    "def log(stats, epoch, train_metrics, valid_metrics):\n",
    "  \"\"\"Logs performance for an epoch.\n",
    "  Args:\n",
    "    stats: A dictionary to be updated with the logged statistics.\n",
    "    epoch: The epoch number.\n",
    "    train_metrics: A dict with the training metrics for this epoch.\n",
    "    valid_metrics: A dict with the validation metrics for this epoch.\n",
    "  \"\"\"\n",
    "  train_loss = train_metrics['loss'] / train_metrics['total']\n",
    "  logging.info('Epoch %02d train loss %.4f valid loss %.4f acc %.2f', epoch + 1,\n",
    "               train_loss, valid_metrics['loss'], valid_metrics['acc'])\n",
    "\n",
    "  # Remember the metrics for later plotting.\n",
    "  stats['train_loss'].append(train_loss.item())\n",
    "  for metric, value in valid_metrics.items():\n",
    "    stats['valid_' + metric].append(value)\n",
    "\n",
    "def train(\n",
    "    model: nn.Model,\n",
    "    learning_rate: float = None,\n",
    "    num_epochs: int = None,\n",
    "    seed: int = None,\n",
    "    model_dir: Text = None,\n",
    "    data_source: Any = None,\n",
    "    batch_size: int = None,\n",
    "    checkpoints_to_keep: int = None,\n",
    "    l2_reg: float = None,\n",
    ") -> Tuple[Dict[Text, Any], nn.Model]:\n",
    "  \"\"\"Training loop.\n",
    "  Args:\n",
    "    model: An initialized model to be trained.\n",
    "    learning_rate: The learning rate.\n",
    "    num_epochs: Train for this many epochs.\n",
    "    seed: Seed for shuffling.\n",
    "    model_dir: Directory to save best model.\n",
    "    data_source: The data source with pre-processed data examples.\n",
    "    batch_size: The batch size to use for training and validation data.\n",
    "    l2_reg: L2 regularization weight.\n",
    "  Returns:\n",
    "    A dict with training statistics and the best model.\n",
    "  \"\"\"\n",
    "  rng = jax.random.PRNGKey(seed)\n",
    "  optimizer = flax.optim.Adam(learning_rate=learning_rate).create(model)\n",
    "  stats = collections.defaultdict(list)\n",
    "  best_score = 0.\n",
    "  train_batches = get_shuffled_batches(\n",
    "      data_source.train_dataset, batch_size=batch_size, seed=seed)\n",
    "  valid_batches = get_batches(\n",
    "      data_source.valid_dataset, batch_size=batch_size)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    train_metrics = collections.defaultdict(float)\n",
    "\n",
    "    # Train for one epoch.\n",
    "    for ex in tfds.as_numpy(train_batches):\n",
    "      inputs, lengths, labels = ex['sentence'], ex['length'], ex['label']\n",
    "      optimizer, loss, rng = train_step(optimizer, inputs, lengths, labels, rng,\n",
    "                                        l2_reg)\n",
    "      train_metrics['loss'] += loss * inputs.shape[0]\n",
    "      train_metrics['total'] += inputs.shape[0]\n",
    "\n",
    "    # Evaluate on validation data. optimizer.target is the updated model.\n",
    "    valid_metrics = evaluate(optimizer.target, valid_batches)\n",
    "    log(stats, epoch, train_metrics, valid_metrics)\n",
    "\n",
    "    # Save a checkpoint if this is the best model so far.\n",
    "    if valid_metrics['acc'] > best_score:\n",
    "      best_score = valid_metrics['acc']\n",
    "      flax.training.checkpoints.save_checkpoint(\n",
    "          model_dir, optimizer.target, epoch + 1, keep=checkpoints_to_keep)\n",
    "\n",
    "  # Done training. Restore best model.\n",
    "  logging.info('Training done! Best validation accuracy: %.2f', best_score)\n",
    "  best_model = flax.training.checkpoints.restore_checkpoint(model_dir, model)\n",
    "\n",
    "  return stats, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_source = SST2DataSource(min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankur.kumar/tf_env/lib/python3.7/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "model = create_model(\n",
    "      3107,\n",
    "      4,\n",
    "      10,\n",
    "      dict(\n",
    "          vocab_size=data_source.vocab_size,\n",
    "          embedding_size=50,\n",
    "          hidden_size=50,\n",
    "          output_size=1,\n",
    "          unk_idx=data_source.unk_idx,\n",
    "          dropout=0.5,\n",
    "          emb_dropout=0.5,\n",
    "          word_dropout_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ankur.kumar/tf_env/lib/python3.7/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d2fbd11c0355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mcheckpoints_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       l2_reg=1e-6)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-fb42effd3eb3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, learning_rate, num_epochs, seed, model_dir, data_source, batch_size, checkpoints_to_keep, l2_reg)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# Train for one epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       optimizer, loss, rng = train_step(optimizer, inputs, lengths, labels, rng,\n",
      "\u001b[0;32m~/tf_env/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_utils.py\u001b[0m in \u001b[0;36m_eager_dataset_iterator\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_eager_dataset_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRaggedTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_env/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_env/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \"\"\"\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_env/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_env/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2104\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m         \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2106\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_stats, model = train(\n",
    "      model,\n",
    "      learning_rate=0.0005,\n",
    "      num_epochs=1,\n",
    "      seed=3107,\n",
    "      model_dir='output_dir',\n",
    "      data_source=data_source,\n",
    "      batch_size=4,\n",
    "      checkpoints_to_keep=0,\n",
    "      l2_reg=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
