{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
    "\n",
    "  Args:\n",
    "    tensor: A tf.Tensor object to find the shape of.\n",
    "    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
    "      specified and the `tensor` has a different rank, and exception will be\n",
    "      thrown.\n",
    "    name: Optional name of the tensor for the error message.\n",
    "\n",
    "  Returns:\n",
    "    A list of dimensions of the shape of tensor. All static dimensions will\n",
    "    be returned as python integers, and dynamic dimensions will be returned\n",
    "    as tf.Tensor scalars.\n",
    "  \"\"\"\n",
    "  if expected_rank is not None:\n",
    "    assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "  shape = tensor.shape.as_list()\n",
    "\n",
    "  non_static_indexes = []\n",
    "  for (index, dim) in enumerate(shape):\n",
    "    if dim is None:\n",
    "      non_static_indexes.append(index)\n",
    "\n",
    "  if not non_static_indexes:\n",
    "    return shape\n",
    "\n",
    "  dyn_shape = tf.shape(tensor)\n",
    "  for index in non_static_indexes:\n",
    "    shape[index] = dyn_shape[index]\n",
    "  return shape\n",
    "\n",
    "def get_initializer(initializer_range=0.02):\n",
    "  \"\"\"Creates a `tf.initializers.truncated_normal` with the given range.\n",
    "\n",
    "  Args:\n",
    "    initializer_range: float, initializer range for stddev.\n",
    "\n",
    "  Returns:\n",
    "    TruncatedNormal initializer with stddev = `initializer_range`.\n",
    "  \"\"\"\n",
    "  return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)\n",
    "\n",
    "def pack_inputs(inputs):\n",
    "  \"\"\"Pack a list of `inputs` tensors to a tuple.\n",
    "\n",
    "  Args:\n",
    "    inputs: a list of tensors.\n",
    "\n",
    "  Returns:\n",
    "    a tuple of tensors. if any input is None, replace it with a special constant\n",
    "    tensor.\n",
    "  \"\"\"\n",
    "  inputs = tf.nest.flatten(inputs)\n",
    "  outputs = []\n",
    "  for x in inputs:\n",
    "    if x is None:\n",
    "      outputs.append(tf.constant(0, shape=[], dtype=tf.int32))\n",
    "    else:\n",
    "      outputs.append(x)\n",
    "  return tuple(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "def print_log(x,y):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLookup(tf.keras.layers.Layer):\n",
    "  \"\"\"Looks up words embeddings for id tensor.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               embedding_size=128,\n",
    "               initializer_range=0.02,\n",
    "               **kwargs):\n",
    "    super(EmbeddingLookup, self).__init__(**kwargs)\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embedding_size = embedding_size\n",
    "    self.initializer_range = initializer_range\n",
    "\n",
    "  def build(self, unused_input_shapes):\n",
    "    \"\"\"Implements build() for the layer.\"\"\"\n",
    "    self.embeddings = self.add_weight(\n",
    "        \"embeddings\",\n",
    "        shape=[self.vocab_size, self.embedding_size],\n",
    "        initializer=get_initializer(self.initializer_range),\n",
    "        dtype=self.dtype)\n",
    "    super(EmbeddingLookup, self).build(unused_input_shapes)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    \"\"\"Implements call() for the layer.\"\"\"\n",
    "    input_shape = get_shape_list(inputs)\n",
    "    print('input_shape :', input_shape)\n",
    "    flat_input = tf.reshape(inputs, [-1])\n",
    "    print('flat_input :', flat_input.shape, flat_input)\n",
    "    output = tf.gather(self.embeddings, flat_input)\n",
    "    print('output :', output.shape)\n",
    "    output = tf.reshape(output, input_shape + [self.embedding_size])\n",
    "    print('output :', output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingPostprocessor(tf.keras.layers.Layer):\n",
    "  \"\"\"Performs various post-processing on a word embedding tensor.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               use_type_embeddings=False,\n",
    "               token_type_vocab_size=None,\n",
    "               use_position_embeddings=True,\n",
    "               max_position_embeddings=512,\n",
    "               hidden_size=4096,\n",
    "               dropout_prob=0.0,\n",
    "               initializer_range=0.02,\n",
    "               initializer=None,\n",
    "               **kwargs):\n",
    "    super(EmbeddingPostprocessor, self).__init__(**kwargs)\n",
    "    self.use_type_embeddings = use_type_embeddings\n",
    "    self.token_type_vocab_size = token_type_vocab_size\n",
    "    self.use_position_embeddings = use_position_embeddings\n",
    "    self.max_position_embeddings = max_position_embeddings\n",
    "    self.hidden_size = hidden_size\n",
    "    self.dropout_prob = dropout_prob\n",
    "    self.initializer_range = initializer_range\n",
    "\n",
    "    if not initializer:\n",
    "      self.initializer = get_initializer(self.initializer_range)\n",
    "    else:\n",
    "      self.initializer = initializer\n",
    "\n",
    "    if self.use_type_embeddings and not self.token_type_vocab_size:\n",
    "      raise ValueError(\"If `use_type_embeddings` is True, then \"\n",
    "                       \"`token_type_vocab_size` must be specified.\")\n",
    "\n",
    "  def build(self, input_shapes):\n",
    "    \"\"\"Implements build() for the layer.\"\"\"\n",
    "    (word_embeddings_shape, _) = input_shapes\n",
    "    width = word_embeddings_shape.as_list()[-1]\n",
    "    self.type_embeddings = None\n",
    "    if self.use_type_embeddings:\n",
    "      self.type_embeddings = self.add_weight(\n",
    "          \"type_embeddings\",\n",
    "          shape=[self.token_type_vocab_size, width],\n",
    "          initializer=get_initializer(self.initializer_range),\n",
    "          dtype=self.dtype)\n",
    "\n",
    "    self.position_embeddings = None\n",
    "    if self.use_position_embeddings:\n",
    "      self.position_embeddings = self.add_weight(\n",
    "          \"position_embeddings\",\n",
    "          shape=[self.max_position_embeddings, width],\n",
    "          initializer=get_initializer(self.initializer_range),\n",
    "          dtype=self.dtype)\n",
    "\n",
    "    self.output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "        name=\"layer_norm\", axis=-1, epsilon=1e-12, dtype=tf.float32)\n",
    "    self.output_dropout = tf.keras.layers.Dropout(rate=self.dropout_prob,\n",
    "                                                  dtype=tf.float32)\n",
    "    self.projection = Dense2DProjection(\n",
    "      output_size=self.hidden_size,\n",
    "      kernel_initializer=get_initializer(self.initializer_range),\n",
    "      activation=None,\n",
    "      # Uses float32 so that gelu activation is done in float32.\n",
    "      fp32_activation=True,\n",
    "      name=\"embedding_hidden_mapping_in\")\n",
    "    super(EmbeddingPostprocessor, self).build(input_shapes)\n",
    "\n",
    "  def __call__(self, word_embeddings, token_type_ids=None, **kwargs):\n",
    "    inputs = pack_inputs([word_embeddings, token_type_ids])\n",
    "    return super(EmbeddingPostprocessor, self).__call__(inputs, **kwargs)\n",
    "\n",
    "  def call(self, inputs, **kwargs):\n",
    "    \"\"\"Implements call() for the layer.\"\"\"\n",
    "    unpacked_inputs = unpack_inputs(inputs)\n",
    "    word_embeddings = unpacked_inputs[0]\n",
    "    token_type_ids = unpacked_inputs[1]\n",
    "    input_shape = get_shape_list(word_embeddings, expected_rank=3)\n",
    "    batch_size = input_shape[0]\n",
    "    seq_length = input_shape[1]\n",
    "    width = input_shape[2]\n",
    "\n",
    "    output = word_embeddings\n",
    "    if self.use_type_embeddings:\n",
    "      flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
    "      one_hot_ids = tf.one_hot(\n",
    "          flat_token_type_ids,\n",
    "          depth=self.token_type_vocab_size,\n",
    "          dtype=self.dtype)\n",
    "      token_type_embeddings = tf.matmul(one_hot_ids, self.type_embeddings)\n",
    "      token_type_embeddings = tf.reshape(token_type_embeddings,\n",
    "                                         [batch_size, seq_length, width])\n",
    "      output += token_type_embeddings\n",
    "\n",
    "    if self.use_position_embeddings:\n",
    "      position_embeddings = tf.expand_dims(\n",
    "          tf.slice(self.position_embeddings, [0, 0], [seq_length, width]),\n",
    "          axis=0)\n",
    "\n",
    "      output += position_embeddings\n",
    "\n",
    "    output = self.output_layer_norm(output)\n",
    "    output = self.output_dropout(output,training=kwargs.get('training', False))\n",
    "\n",
    "    projected_output = self.projection(output)\n",
    "\n",
    "    return projected_output\n",
    "\n",
    "class Dense2DProjection(tf.keras.layers.Layer):\n",
    "  \"\"\"A 2D projection layer with tf.einsum implementation.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               output_size,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=\"zeros\",\n",
    "               activation=None,\n",
    "               fp32_activation=False,\n",
    "               **kwargs):\n",
    "    super(Dense2DProjection, self).__init__(**kwargs)\n",
    "    self.output_size = output_size\n",
    "    self.kernel_initializer = kernel_initializer\n",
    "    self.bias_initializer = bias_initializer\n",
    "    self.activation = activation\n",
    "    self.fp32_activation = fp32_activation\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    \"\"\"Implements build() for the layer.\"\"\"\n",
    "    dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
    "    if not (dtype.is_floating or dtype.is_complex):\n",
    "      raise TypeError(\"Unable to build `Dense2DProjection` layer with \"\n",
    "                      \"non-floating point (and non-complex) \"\n",
    "                      \"dtype %s\" % (dtype,))\n",
    "    input_shape = tf.TensorShape(input_shape)\n",
    "    if tf.compat.dimension_value(input_shape[-1]) is None:\n",
    "      raise ValueError(\"The last dimension of the inputs to \"\n",
    "                       \"`Dense2DProjection` should be defined. \"\n",
    "                       \"Found `None`.\")\n",
    "    last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "    self.input_spec = tf.keras.layers.InputSpec(min_ndim=3, axes={-1: last_dim})\n",
    "    self.kernel = self.add_weight(\n",
    "        \"kernel\",\n",
    "        shape=[last_dim, self.output_size],\n",
    "        initializer=self.kernel_initializer,\n",
    "        dtype=self.dtype,\n",
    "        trainable=True)\n",
    "    self.bias = self.add_weight(\n",
    "        \"bias\",\n",
    "        shape=[self.output_size],\n",
    "        initializer=self.bias_initializer,\n",
    "        dtype=self.dtype,\n",
    "        trainable=True)\n",
    "    super(Dense2DProjection, self).build(input_shape)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    \"\"\"Implements call() for Dense2DProjection.\n",
    "\n",
    "    Args:\n",
    "      inputs: float Tensor of shape [batch, from_seq_length,\n",
    "        num_attention_heads, size_per_head].\n",
    "\n",
    "    Returns:\n",
    "      A 3D Tensor.\n",
    "    \"\"\"\n",
    "    ret = tf.einsum(\"abc,cd->abd\", inputs, self.kernel)\n",
    "    ret += self.bias\n",
    "    if self.activation is not None:\n",
    "      if self.dtype == tf.float16 and self.fp32_activation:\n",
    "        ret = tf.cast(ret, tf.float32)\n",
    "      return self.activation(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape : [<tf.Tensor 'embedding_lookup_13/strided_slice:0' shape=() dtype=int32>, 32]\n",
      "flat_input : (None,) Tensor(\"embedding_lookup_13/Reshape:0\", shape=(None,), dtype=int32)\n",
      "output : (None, 128)\n",
      "output : (None, 32, 128)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'unpack_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-502d81894b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m             shape=(32,), dtype=tf.int32, name='input_word_ids')\n\u001b[1;32m      3\u001b[0m \u001b[0membedding_lookup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingLookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0membedding_postprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingPostprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-9121a12bf703>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, word_embeddings, token_type_ids, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbeddingPostprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-9121a12bf703>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;34m\"\"\"Implements call() for the layer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpacked_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpacked_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unpack_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "input_layer = tf.keras.layers.Input(\n",
    "            shape=(32,), dtype=tf.int32, name='input_word_ids')\n",
    "embedding_lookup = EmbeddingLookup(100,128)(input_layer)\n",
    "embedding_postprocessor = EmbeddingPostprocessor(True,2)(embedding_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(input_layer, embedding_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_lookup_8 (Embeddin (None, 32, 128)           12800     \n",
      "=================================================================\n",
      "Total params: 12,800\n",
      "Trainable params: 12,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
