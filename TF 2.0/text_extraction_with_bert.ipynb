{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re\nimport json\nimport string\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\n\nmax_len = 384\nconfiguration = BertConfig()  # default paramters and configuration for BERT","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nsave_path = \"bert_base_uncased/\"\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\nslow_tokenizer.save_pretrained(save_path)\n\n# Load the fast tokenizer from saved file\ntokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)","execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f82c18ce20a48faa4efd676365c59e2"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\ntrain_path = keras.utils.get_file(\"train.json\", train_data_url)\neval_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\neval_path = keras.utils.get_file(\"eval.json\", eval_data_url)","execution_count":4,"outputs":[{"output_type":"stream","text":"Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n30294016/30288272 [==============================] - 0s 0us/step\nDownloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n4857856/4854279 [==============================] - 0s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SquadExample:\n    def __init__(self, question, context, start_char_idx, answer_text, all_answers):\n        self.question = question\n        self.context = context\n        self.start_char_idx = start_char_idx\n        self.answer_text = answer_text\n        self.all_answers = all_answers\n        self.skip = False\n\n    def preprocess(self):\n        context = self.context\n        question = self.question\n        answer_text = self.answer_text\n        start_char_idx = self.start_char_idx\n\n        # Clean context, answer and question\n        context = \" \".join(str(context).split())\n        question = \" \".join(str(question).split())\n        answer = \" \".join(str(answer_text).split())\n\n        # Find end character index of answer in context\n        end_char_idx = start_char_idx + len(answer)\n        if end_char_idx >= len(context):\n            self.skip = True\n            return\n\n        # Mark the character indexes in context that are in answer\n        is_char_in_ans = [0] * len(context)\n        for idx in range(start_char_idx, end_char_idx):\n            is_char_in_ans[idx] = 1\n\n        # Tokenize context\n        tokenized_context = tokenizer.encode(context)\n\n        # Find tokens that were created from answer characters\n        ans_token_idx = []\n        for idx, (start, end) in enumerate(tokenized_context.offsets):\n            if sum(is_char_in_ans[start:end]) > 0:\n                ans_token_idx.append(idx)\n\n        if len(ans_token_idx) == 0:\n            self.skip = True\n            return\n\n        # Find start and end token index for tokens from answer\n        start_token_idx = ans_token_idx[0]\n        end_token_idx = ans_token_idx[-1]\n\n        # Tokenize question\n        tokenized_question = tokenizer.encode(question)\n\n        # Create inputs\n        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n            tokenized_question.ids[1:]\n        )\n        attention_mask = [1] * len(input_ids)\n\n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        padding_length = max_len - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n\n        self.input_ids = input_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n        self.start_token_idx = start_token_idx\n        self.end_token_idx = end_token_idx\n        self.context_token_to_char = tokenized_context.offsets\n\n\nwith open(train_path) as f:\n    raw_train_data = json.load(f)\n\nwith open(eval_path) as f:\n    raw_eval_data = json.load(f)\n\n\ndef create_squad_examples(raw_data):\n    squad_examples = []\n    for i in tqdm.tqdm(range(len(raw_data[\"data\"]))):\n        #for item in raw_data[\"data\"]:\n        item = raw_data[\"data\"][i]\n        for para in item[\"paragraphs\"]:\n            context = para[\"context\"]\n            for qa in para[\"qas\"]:\n                question = qa[\"question\"]\n                answer_text = qa[\"answers\"][0][\"text\"]\n                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n                squad_eg = SquadExample(\n                    question, context, start_char_idx, answer_text, all_answers\n                )\n                squad_eg.preprocess()\n                squad_examples.append(squad_eg)\n    return squad_examples\n\n\ndef create_inputs_targets(squad_examples):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": [],\n        \"start_token_idx\": [],\n        \"end_token_idx\": [],\n    }\n    for item in squad_examples:\n        if item.skip == False:\n            for key in dataset_dict:\n                dataset_dict[key].append(getattr(item, key))\n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n    return x, y\n","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_squad_examples = create_squad_examples(raw_train_data)","execution_count":21,"outputs":[{"output_type":"stream","text":"100%|██████████| 442/442 [01:26<00:00,  5.12it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, y_train = create_inputs_targets(train_squad_examples)\nprint(f\"{len(train_squad_examples)} training points created.\")","execution_count":22,"outputs":[{"output_type":"stream","text":"87599 training points created.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_squad_examples = create_squad_examples(raw_eval_data)\nx_eval, y_eval = create_inputs_targets(eval_squad_examples)\nprint(f\"{len(eval_squad_examples)} evaluation points created.\")","execution_count":23,"outputs":[{"output_type":"stream","text":"100%|██████████| 48/48 [00:10<00:00,  4.45it/s]\n","name":"stderr"},{"output_type":"stream","text":"10570 evaluation points created.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    ## BERT encoder\n    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n\n    ## QA Model\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_tpu = True\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model()\nelse:\n    model = create_model()\n\nmodel.summary()","execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3803dad336694d71b80ad6456db48401"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"653a22569d994ea8b1333f42e00ed973"}},"metadata":{}},{"output_type":"stream","text":"\nModel: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 384)]        0                                            \n__________________________________________________________________________________________________\ninput_3 (InputLayer)            [(None, 384)]        0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 384)]        0                                            \n__________________________________________________________________________________________________\ntf_bert_model (TFBertModel)     ((None, 384, 768), ( 109482240   input_1[0][0]                    \n__________________________________________________________________________________________________\nstart_logit (Dense)             (None, 384, 1)       768         tf_bert_model[0][0]              \n__________________________________________________________________________________________________\nend_logit (Dense)               (None, 384, 1)       768         tf_bert_model[0][0]              \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 384)          0           start_logit[0][0]                \n__________________________________________________________________________________________________\nflatten_1 (Flatten)             (None, 384)          0           end_logit[0][0]                  \n__________________________________________________________________________________________________\nactivation_7 (Activation)       (None, 384)          0           flatten[0][0]                    \n__________________________________________________________________________________________________\nactivation_8 (Activation)       (None, 384)          0           flatten_1[0][0]                  \n==================================================================================================\nTotal params: 109,483,776\nTrainable params: 109,483,776\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_text(text):\n    text = text.lower()\n\n    # Remove punctuations\n    exclude = set(string.punctuation)\n    text = \"\".join(ch for ch in text if ch not in exclude)\n\n    # Remove articles\n    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n    text = re.sub(regex, \" \", text)\n\n    # Remove extra white space\n    text = \" \".join(text.split())\n    return text\n\n\nclass ExactMatch(keras.callbacks.Callback):\n    \"\"\"\n    Each `SquadExample` object contains the character level offsets for each token\n    in its input paragraph. We use them to get back the span of text corresponding\n    to the tokens between our predicted start and end tokens.\n    All the ground-truth answers are also present in each `SquadExample` object.\n    We calculate the percentage of data points where the span of text obtained\n    from model predictions matches one of the ground-truth answers.\n    \"\"\"\n\n    def __init__(self, x_eval, y_eval):\n        self.x_eval = x_eval\n        self.y_eval = y_eval\n\n    def on_epoch_end(self, epoch, logs=None):\n        pred_start, pred_end = self.model.predict(self.x_eval)\n        count = 0\n        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n            squad_eg = eval_examples_no_skip[idx]\n            offsets = squad_eg.context_token_to_char\n            start = np.argmax(start)\n            end = np.argmax(end)\n            if start >= len(offsets):\n                continue\n            pred_char_start = offsets[start][0]\n            if end < len(offsets):\n                pred_char_end = offsets[end][1]\n                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n            else:\n                pred_ans = squad_eg.context[pred_char_start:]\n\n            normalized_pred_ans = normalize_text(pred_ans)\n            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n            if normalized_pred_ans in normalized_true_ans:\n                count += 1\n        acc = count / len(self.y_eval[0])\n        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_x_train = [x_train[0][0:86080], x_train[1][0:86080], x_train[2][0:86080]]","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_y_train = [y_train[0][0:86080], y_train[1][0:86080]]","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exact_match_callback = ExactMatch(x_eval, y_eval)\nmodel.fit(\n    new_x_train,\n    new_y_train,\n    epochs=1,  # For demonstration, 3 epochs are recommended\n    verbose=2,\n    batch_size=64,\n    callbacks=[exact_match_callback],\n)","execution_count":48,"outputs":[{"output_type":"stream","text":"Train on 86080 samples\n\nepoch=1, exact match score=0.78\n86080/86080 - 303s - loss: 2.5454 - activation_7_loss: 1.3344 - activation_8_loss: 1.2109\n","name":"stdout"},{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fcb145467d0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"exact_match_callback = ExactMatch(x_eval, y_eval)\nmodel.fit(\n    new_x_train,\n    new_y_train,\n    epochs=3,  # For demonstration, 3 epochs are recommended\n    verbose=1,\n    batch_size=64,\n    callbacks=[exact_match_callback],\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}