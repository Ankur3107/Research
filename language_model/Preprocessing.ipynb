{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyObject:\n",
    "    def __init__(self,**kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "FLAGS=DummyObject(do_whole_word_mask=True,\n",
    "                 ngram=3,\n",
    "                 favor_shorter_ngram=False,\n",
    "                 do_permutation=False,\n",
    "                 random_seed=3107,\n",
    "                 max_predictions_per_seq=5,\n",
    "                 masked_lm_prob=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.Random(FLAGS.random_seed)\n",
    "vocab_words = list(tokenizer.vocab.keys())\n",
    "tokens = tokenizer.tokenize('hi how are you doing, i am fine and how about you. you say. i am also good')\n",
    "max_predictions_per_seq = FLAGS.max_predictions_per_seq\n",
    "masked_lm_prob = FLAGS.masked_lm_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_start_piece_bert(piece):\n",
    "  \"\"\"Check if the current word piece is the starting piece (BERT).\"\"\"\n",
    "  # When a word has been split into\n",
    "  # WordPieces, the first token does not have any marker and any subsequence\n",
    "  # tokens are prefixed with ##. So whenever we see the ## token, we\n",
    "  # append it to the previous set of word indexes.\n",
    "  return not six.ensure_str(piece).startswith(\"##\")\n",
    "\n",
    "\n",
    "def is_start_piece(piece):\n",
    "    return _is_start_piece_bert(piece)\n",
    "\n",
    "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
    "                                 max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "  cand_indexes = []\n",
    "  # Note(mingdachen): We create a list for recording if the piece is\n",
    "  # the starting piece of current token, where 1 means true, so that\n",
    "  # on-the-fly whole word masking is possible.\n",
    "  token_boundary = [0] * len(tokens)\n",
    "\n",
    "  for (i, token) in enumerate(tokens):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "      token_boundary[i] = 1\n",
    "      continue\n",
    "    # Whole Word Masking means that if we mask all of the wordpieces\n",
    "    # corresponding to an original word.\n",
    "    #\n",
    "    # Note that Whole Word Masking does *not* change the training code\n",
    "    # at all -- we still predict each WordPiece independently, softmaxed\n",
    "    # over the entire vocabulary.\n",
    "    if (FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and\n",
    "        not is_start_piece(token)):\n",
    "      cand_indexes[-1].append(i)\n",
    "    else:\n",
    "      cand_indexes.append([i])\n",
    "      if is_start_piece(token):\n",
    "        token_boundary[i] = 1\n",
    "\n",
    "  output_tokens = list(tokens)\n",
    "\n",
    "  masked_lm_positions = []\n",
    "  masked_lm_labels = []\n",
    "\n",
    "  if masked_lm_prob == 0:\n",
    "    return (output_tokens, masked_lm_positions,\n",
    "            masked_lm_labels, token_boundary)\n",
    "\n",
    "  num_to_predict = min(max_predictions_per_seq,\n",
    "                       max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "  # Note(mingdachen):\n",
    "  # By default, we set the probilities to favor longer ngram sequences.\n",
    "  ngrams = np.arange(1, FLAGS.ngram + 1, dtype=np.int64)\n",
    "  pvals = 1. / np.arange(1, FLAGS.ngram + 1)\n",
    "  pvals /= pvals.sum(keepdims=True)\n",
    "\n",
    "  if FLAGS.favor_shorter_ngram:\n",
    "    pvals = pvals[::-1]\n",
    "\n",
    "  ngram_indexes = []\n",
    "  for idx in range(len(cand_indexes)):\n",
    "    ngram_index = []\n",
    "    for n in ngrams:\n",
    "      ngram_index.append(cand_indexes[idx:idx+n])\n",
    "    ngram_indexes.append(ngram_index)\n",
    "\n",
    "  rng.shuffle(ngram_indexes)\n",
    "\n",
    "  masked_lms = []\n",
    "  covered_indexes = set()\n",
    "  for cand_index_set in ngram_indexes:\n",
    "    if len(masked_lms) >= num_to_predict:\n",
    "      break\n",
    "    if not cand_index_set:\n",
    "      continue\n",
    "    # Note(mingdachen):\n",
    "    # Skip current piece if they are covered in lm masking or previous ngrams.\n",
    "    for index_set in cand_index_set[0]:\n",
    "      for index in index_set:\n",
    "        if index in covered_indexes:\n",
    "          continue\n",
    "\n",
    "    n = np.random.choice(ngrams[:len(cand_index_set)],\n",
    "                         p=pvals[:len(cand_index_set)] /\n",
    "                         pvals[:len(cand_index_set)].sum(keepdims=True))\n",
    "    index_set = sum(cand_index_set[n - 1], [])\n",
    "    n -= 1\n",
    "    # Note(mingdachen):\n",
    "    # Repeatedly looking for a candidate that does not exceed the\n",
    "    # maximum number of predictions by trying shorter ngrams.\n",
    "    while len(masked_lms) + len(index_set) > num_to_predict:\n",
    "      if n == 0:\n",
    "        break\n",
    "      index_set = sum(cand_index_set[n - 1], [])\n",
    "      n -= 1\n",
    "    # If adding a whole-word mask would exceed the maximum number of\n",
    "    # predictions, then just skip this candidate.\n",
    "    if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "      continue\n",
    "    is_any_index_covered = False\n",
    "    for index in index_set:\n",
    "      if index in covered_indexes:\n",
    "        is_any_index_covered = True\n",
    "        break\n",
    "    if is_any_index_covered:\n",
    "      continue\n",
    "    for index in index_set:\n",
    "      covered_indexes.add(index)\n",
    "\n",
    "      masked_token = None\n",
    "      # 80% of the time, replace with [MASK]\n",
    "      if rng.random() < 0.8:\n",
    "        masked_token = \"[MASK]\"\n",
    "      else:\n",
    "        # 10% of the time, keep original\n",
    "        if rng.random() < 0.5:\n",
    "          masked_token = tokens[index]\n",
    "        # 10% of the time, replace with random word\n",
    "        else:\n",
    "          masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "\n",
    "      output_tokens[index] = masked_token\n",
    "\n",
    "      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "  assert len(masked_lms) <= num_to_predict\n",
    "\n",
    "  rng.shuffle(ngram_indexes)\n",
    "\n",
    "  select_indexes = set()\n",
    "  if FLAGS.do_permutation:\n",
    "    for cand_index_set in ngram_indexes:\n",
    "      if len(select_indexes) >= num_to_predict:\n",
    "        break\n",
    "      if not cand_index_set:\n",
    "        continue\n",
    "      # Note(mingdachen):\n",
    "      # Skip current piece if they are covered in lm masking or previous ngrams.\n",
    "      for index_set in cand_index_set[0]:\n",
    "        for index in index_set:\n",
    "          if index in covered_indexes or index in select_indexes:\n",
    "            continue\n",
    "\n",
    "      n = np.random.choice(ngrams[:len(cand_index_set)],\n",
    "                           p=pvals[:len(cand_index_set)] /\n",
    "                           pvals[:len(cand_index_set)].sum(keepdims=True))\n",
    "      index_set = sum(cand_index_set[n - 1], [])\n",
    "      n -= 1\n",
    "\n",
    "      while len(select_indexes) + len(index_set) > num_to_predict:\n",
    "        if n == 0:\n",
    "          break\n",
    "        index_set = sum(cand_index_set[n - 1], [])\n",
    "        n -= 1\n",
    "      # If adding a whole-word mask would exceed the maximum number of\n",
    "      # predictions, then just skip this candidate.\n",
    "      if len(select_indexes) + len(index_set) > num_to_predict:\n",
    "        continue\n",
    "      is_any_index_covered = False\n",
    "      for index in index_set:\n",
    "        if index in covered_indexes or index in select_indexes:\n",
    "          is_any_index_covered = True\n",
    "          break\n",
    "      if is_any_index_covered:\n",
    "        continue\n",
    "      for index in index_set:\n",
    "        select_indexes.add(index)\n",
    "    assert len(select_indexes) <= num_to_predict\n",
    "\n",
    "    select_indexes = sorted(select_indexes)\n",
    "    permute_indexes = list(select_indexes)\n",
    "    rng.shuffle(permute_indexes)\n",
    "    orig_token = list(output_tokens)\n",
    "\n",
    "    for src_i, tgt_i in zip(select_indexes, permute_indexes):\n",
    "      output_tokens[src_i] = orig_token[tgt_i]\n",
    "      masked_lms.append(MaskedLmInstance(index=src_i, label=orig_token[src_i]))\n",
    "\n",
    "  masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "  for p in masked_lms:\n",
    "    masked_lm_positions.append(p.index)\n",
    "    masked_lm_labels.append(p.label)\n",
    "  return (output_tokens, masked_lm_positions, masked_lm_labels, token_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['hi',\n",
       "  'how',\n",
       "  'are',\n",
       "  'you',\n",
       "  'doing',\n",
       "  ',',\n",
       "  'i',\n",
       "  '[MASK]',\n",
       "  'fine',\n",
       "  'and',\n",
       "  'how',\n",
       "  'about',\n",
       "  'you',\n",
       "  '.',\n",
       "  'you',\n",
       "  'say',\n",
       "  '.',\n",
       "  'i',\n",
       "  'am',\n",
       "  'also',\n",
       "  '[MASK]'],\n",
       " [3, 7, 20],\n",
       " ['you', 'am', 'good'],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_masked_lm_predictions(tokens, masked_lm_prob,max_predictions_per_seq, vocab_words, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('tf2_env': venv)",
   "language": "python",
   "name": "python37564bittf2envvenv9baad5f63bf544adb456767f347f0830"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
